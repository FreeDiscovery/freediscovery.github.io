

.. _sphx_glr_examples_python_clustering.py:


Clustering Example [Python API]
-------------------------------

An example of clustering using the Python API





.. rst-class:: sphx-glr-script-out

 Out::

    0. Load Dataset

    Warning: downloading dataset treclegal09_2k_subset (2.8 MB) !

    File /tmp/treclegal09_2k_subset.tar.gz downloaded!
    Archive extracted!

    1. Feature extraction (non hashed)


    2. Computing LSI


    3. Document Clustering (LSI + K-Means)

        .. computed in 0.6s
       N_documents                                      cluster_names
    1          571  [teneo, ect, shackleton, shackleton_sara, grou...
    2          385   [dasovich, rewrite, mail, enron, alias, address]
    3          342        [tana, jones, ect, test, recipients, group]
    4          299               [tenet, normal, fri, nov, test, mon]
    7          209       [tue, haedicke, eb3325, meet, normal, tenet]
    9          188  [energy, services, customer, power, enrononlin...
    0          168  [enron_development, kincannon, advice, update,...
    6          142               [wed, tenet, normal, oct, test, gmt]
    8           97        [sanders, nov, normal, lunch, meeting, tue]
    5           64       [migration, outlook, team, mtg, normal, oct]

    4. Document Clustering (LSI + Ward Hierarchical Clustering)

        .. computed in 1.2s
       N_documents                                      cluster_names
    0         1110  [ect, enron, shackleton, enron_development, te...
    1          516       [tenet, normal, test, oct, gmt, sensitivity]
    3          176      [tana, jones, enron, advice, trading, online]
    4          155   [haedicke, ect, lon, forster, group, recipients]
    2          122       [dasovich, alias, berkeley, haas, edu, test]
    6          100  [nemec, doc, townsend, bump, nemec_gerald, sou...
    5           92   [sanders, nov, normal, lunch, sent, sensitivity]
    8           74  [ricafrente, ricafrente_david, eb3325, teneo, ...
    9           64  [migration, outlook, team, mtg, normal, sensit...
    7           56    [rewrite, server, address, smtp, mail, virtual]




|


.. code-block:: python


    import pandas as pd
    from freediscovery.text import FeatureVectorizer
    from freediscovery.cluster import _ClusteringWrapper
    from freediscovery.lsi import _LSIWrapper
    from freediscovery.datasets import load_dataset
    from freediscovery.tests.run_suite import check_cache
    from time import time

    pd.options.display.float_format = '{:,.3f}'.format

    dataset_name = "treclegal09_2k_subset"
    cache_dir = check_cache()


    print("0. Load Dataset")

    ds = load_dataset(dataset_name, cache_dir=cache_dir)


    print("\n1. Feature extraction (non hashed)\n")

    n_features = 30000
    fe = FeatureVectorizer(cache_dir=cache_dir)
    uuid = fe.preprocess(ds['data_dir'],
                         n_features=n_features, use_hashing=False,
                         use_idf=True, stop_words='english')
    uuid, filenames = fe.transform()




    n_clusters = 10
    n_top_words = 6
    lsi_components = 50


    def repr_clustering(_labels, _terms):
        out = []
        for ridx, row in enumerate(_terms):
            out.append({'cluster_names': row, 'N_documents': (_labels == ridx).sum()})
        out = pd.DataFrame(out).sort_values('N_documents', ascending=False)
        return out

    print("\n2. Computing LSI\n")
    lsi = _LSIWrapper(cache_dir=cache_dir, parent_id=uuid)
    lsi_res, exp_var = lsi.fit_transform(n_components=lsi_components)  # TODO unused variables



    print("\n3. Document Clustering (LSI + K-Means)\n")

    cat = _ClusteringWrapper(cache_dir=cache_dir, parent_id=lsi.mid)

    t0 = time()
    labels, tree  = cat.k_means(n_clusters)
    terms = cat.compute_labels(n_top_words=n_top_words)
    t1 = time()

    print('    .. computed in {:.1f}s'.format(t1 - t0))
    print(repr_clustering(labels, terms))


    print('\n4. Document Clustering (LSI + Ward Hierarchical Clustering)\n')


    t0 = time()
    labels, tree = cat.ward_hc(n_clusters,
                               n_neighbors=5   # this is the connectivity constraint
                               )
    terms = cat.compute_labels(n_top_words=n_top_words)
    t1 = time()

    print('    .. computed in {:.1f}s'.format(t1 - t0))
    print(repr_clustering(labels, terms))

**Total running time of the script:** ( 0 minutes  26.690 seconds)



.. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: python_clustering.py <python_clustering.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: python_clustering.ipynb <python_clustering.ipynb>`

.. rst-class:: sphx-glr-signature

    `Generated by Sphinx-Gallery <http://sphinx-gallery.readthedocs.io>`_
