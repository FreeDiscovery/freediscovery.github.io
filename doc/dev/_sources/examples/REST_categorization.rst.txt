

.. _sphx_glr_examples_REST_categorization.py:


Categorization Example [REST API]
---------------------------------

An example to illustrate binary categorizaiton with FreeDiscovery





.. rst-class:: sphx-glr-script-out

 Out::

    0. Load the test dataset
     GET http://localhost:5001/api/v0/datasets/treclegal09_2k_subset

    1.a Load dataset and initalize feature extraction
     POST http://localhost:5001/api/v0/feature-extraction
       => received [u'id', u'filenames']
       => dsid = f924b48e84ee4828

    1.b Start feature extraction (in the background)
     POST http://localhost:5001/api/v0/feature-extraction/f924b48e84ee4828

    1.c Monitor feature extraction progress
     GET http://localhost:5001/api/v0/feature-extraction/f924b48e84ee4828

    1.d. check the parameters of the extracted features
     GET http://localhost:5001/api/v0/feature-extraction/f924b48e84ee4828
         - binary: False
         - n_jobs: 1
         - stop_words: None
         - use_hashing: True
         - min_df: 0.0
         - n_samples: 2465
         - analyzer: word
         - ngram_range: [1, 1]
         - max_df: 1.0
         - chunk_size: 5000
         - use_idf: False
         - data_dir: ../freediscovery_shared/treclegal09_2k_subset/data
         - sublinear_tf: True
         - n_samples_processed: 2465
         - n_features: 100001
         - norm: None

    2. Calculate LSI
    POST http://localhost:5001/api/v0/lsi/
      => LSI model id = 9e3486196578474a
      => SVD decomposition with 100 dimensions explaining 99.79 % variabilty of the data

    3.a. Train the categorization model
       5 relevant, 63 non-relevant files
    ================================================================================ 
                LinearSVC   
     ================================================================================
     POST http://localhost:5001/api/v0/categorization/
     Training...
         => model id = 1c37b2f58b854a4d
        => Training scores: MAP = 0.537, ROC-AUC = nan

    3.b. Check the parameters used in the categorization model
     GET http://localhost:5001/api/v0/categorization/1c37b2f58b854a4d
         - method: LinearSVC
         - options: {'loss': 'squared_hinge', 'C': 1.0, 'verbose': 0, 'intercept_scaling': 1, 'fit_intercept': True, 'max_iter': 1000, 'penalty': 'l2', 'multi_class': 'ovr', 'random_state': None, 'dual': True, 'tol': 0.0001, 'class_weight': None}

    3.c Categorize the complete dataset with this model
     GET http://localhost:5001/api/v0/categorization/1c37b2f58b854a4d/predict

    3.d Test categorization accuracy
             using ../freediscovery_shared/treclegal09_2k_subset/ground_truth_file.txt
    POST http://localhost:5001/api/v0/categorization/1c37b2f58b854a4d/test
        => Test scores: MAP = 0.502, ROC-AUC = nan
    ================================================================================ 
                NearestNeighbor  + LSI 
     ================================================================================
     POST http://localhost:5001/api/v0/categorization/
     Training...
         => model id = aa4dc2d8fde94677
        => Training scores: MAP = 0.537, ROC-AUC = nan

    3.b. Check the parameters used in the categorization model
     GET http://localhost:5001/api/v0/categorization/aa4dc2d8fde94677
         - method: NearestNeighbor
         - options: {'ranking': None, 'n_jobs': None, 'radius': None, 'leaf_size': None, 'algorithm': None}

    3.c Categorize the complete dataset with this model
     GET http://localhost:5001/api/v0/categorization/aa4dc2d8fde94677/predict

    3.d Test categorization accuracy
             using ../freediscovery_shared/treclegal09_2k_subset/ground_truth_file.txt
    POST http://localhost:5001/api/v0/categorization/aa4dc2d8fde94677/test
        => Test scores: MAP = 0.502, ROC-AUC = nan

           dist_n  dist_p  ind_n  ind_p  prediction
    0      0.434   0.406     52      3       0.899
    1      1.131   1.295     53      4       0.676
    2      0.170   0.168     21      0       0.958
    3      0.123   0.000     21      0       1.000
    4      0.240   0.231     64      0       0.942
    5      0.212   0.184     21      0       0.954
    6      0.268   0.228      5      0       0.943
    7      0.176   0.138     37      0       0.966
    8      0.172   0.157     37      0       0.961
    9      0.000   0.205      5      0       0.949
    10     0.177   0.097     21      0       0.976
    11     1.163   1.352     58      4       0.662
    12     1.181   1.311     58      4       0.672
    13     1.148   1.327     55      4       0.668
    14     0.156   1.227     55      1       0.693
    15     0.427   0.859     12      1       0.785
    16     1.308   1.390     58      4       0.653
    17     1.253   1.317     58      4       0.671
    18     0.843   1.087     12      1       0.728
    19     0.817   1.217     12      1       0.696
    20     0.412   0.914     12      1       0.771
    21     0.727   1.358     63      3       0.661
    22     1.034   1.311     58      4       0.672
    23     1.042   1.313     58      4       0.672
    24     0.818   1.088     12      4       0.728
    25     1.208   1.414     63      4       0.647
    26     1.209   1.358     58      4       0.661
    27     1.012   1.215     26      2       0.696
    28     1.299   1.397     23      4       0.651
    29     1.260   1.324     58      4       0.669
    ...      ...     ...    ...    ...         ...
    2435   1.301   1.365     59      1       0.659
    2436   0.226   0.206     21      0       0.949
    2437   0.429   0.689     47      1       0.828
    2438   0.412   0.630     47      0       0.843
    2439   0.192   0.225      5      0       0.944
    2440   0.389   0.643     47      1       0.839
    2441   0.176   0.212      5      0       0.947
    2442   0.391   0.622     47      0       0.845
    2443   0.254   0.245     52      0       0.939
    2444   0.468   0.672     47      4       0.832
    2445   0.295   0.266     37      0       0.934
    2446   0.469   0.682     35      0       0.829
    2447   0.224   0.186     64      0       0.953
    2448   0.345   0.636     66      1       0.841
    2449   0.170   0.100     37      0       0.975
    2450   0.391   0.571     35      0       0.857
    2451   0.000   0.249     67      0       0.938
    2452   0.466   0.671     47      0       0.832
    2453   0.287   0.399     51      0       0.900
    2454   0.564   0.672     66      1       0.832
    2455   0.281   0.645     47      0       0.839
    2456   0.112   0.070     21      0       0.982
    2457   0.433   0.633     47      0       0.842
    2458   0.228   0.187     21      0       0.953
    2459   0.389   1.078     42      2       0.731
    2460   0.851   1.033     42      3       0.742
    2461   0.954   1.010     13      2       0.747
    2462   0.900   1.222     39      0       0.695
    2463   0.983   1.060     42      2       0.735
    2464   0.919   1.117     42      0       0.721

    [2465 rows x 5 columns]

    5.a Delete the extracted features (and LSI decomposition)
     DELETE http://localhost:5001/api/v0/feature-extraction/f924b48e84ee4828




|


.. code-block:: python


    from __future__ import print_function

    from time import time, sleep
    from multiprocessing import Process
    import requests
    import pandas as pd

    pd.options.display.float_format = '{:,.3f}'.format
    pd.options.display.expand_frame_repr = False

    dataset_name = "treclegal09_2k_subset"     # see list of available datasets

    BASE_URL = "http://localhost:5001/api/v0"  # FreeDiscovery server URL

    if __name__ == '__main__':

        print(" 0. Load the test dataset")
        url = BASE_URL + '/datasets/{}'.format(dataset_name)
        print(" GET", url)
        res = requests.get(url).json()

        # To use a custom dataset, simply specify the following variables
        data_dir = res['data_dir']
        seed_filenames = res['seed_filenames']
        seed_y = res['seed_y']
        ground_truth_file = res['ground_truth_file']  # (optional)


        # 1. Feature extraction

        print("\n1.a Load dataset and initalize feature extraction")
        url = BASE_URL + '/feature-extraction'
        print(" POST", url)
        res = requests.post(url, json={'data_dir': data_dir,
                                       'use_hashing': True}).json()

        dsid = res['id']
        print("   => received {}".format(list(res.keys())))
        print("   => dsid = {}".format(dsid))

        print("\n1.b Start feature extraction (in the background)")

        # Make this call in a background process (there should be a better way of doing it)
        url = BASE_URL+'/feature-extraction/{}'.format(dsid)
        print(" POST", url)
        p = Process(target=requests.post, args=(url,))
        p.start()
        sleep(5.0) # wait a bit for the processing to start

        print('\n1.c Monitor feature extraction progress')
        url = BASE_URL+'/feature-extraction/{}'.format(dsid)
        print(" GET", url)

        t0 = time()
        while True:
            res = requests.get(url)
            if res.status_code == 520:
                p.terminate()
                raise ValueError('Processing did not start')
            elif res.status_code == 200:
                break # processing finished
            data = res.json()
            print('     ... {}k/{}k files processed in {:.1f} min'.format(
                        data['n_samples_processed']//1000, data['n_samples']//1000, (time() - t0)/60.))
            sleep(15.0)

        p.terminate()  # just in case, should not be necessary


        print("\n1.d. check the parameters of the extracted features")
        url = BASE_URL + '/feature-extraction/{}'.format(dsid)
        print(' GET', url)
        res = requests.get(url).json()

        print('\n'.join(['     - {}: {}'.format(key, val) for key, val in res.items() \
                                                          if "filenames" not in key]))

        method = BASE_URL + "/feature-extraction/{}/index".format(dsid)
        res = requests.get(method, data={'filenames': seed_filenames})
        seed_index = res.json()['index']


        # 3. Document categorization with LSI (used for Nearest Neighbors method)

        print("\n2. Calculate LSI")

        url = BASE_URL + '/lsi/'
        print("POST", url)

        n_components = 100
        res = requests.post(url,
                            json={'n_components': n_components,
                                  'parent_id': dsid
                                  }).json()

        lsi_id = res['id']
        print('  => LSI model id = {}'.format(lsi_id))
        print('  => SVD decomposition with {} dimensions explaining {:.2f} % variabilty of the data'.format(
                                n_components, res['explained_variance']*100))


        # 3. Document categorization

        print("\n3.a. Train the categorization model")
        print("   {} relevant, {} non-relevant files".format(seed_y.count(1), seed_y.count(0)))

        for method, use_lsi in [('LinearSVC', False),
                                ('NearestNeighbor', True)]:

            print('='*80, '\n', ' '*10,
                  method, " + LSI" if use_lsi else ' ', '\n', '='*80)
            if use_lsi:
                # Categorization with the previously created LSI model
                parent_id = lsi_id
            else:
                # Categorization with original text features
                parent_id = dsid

            url = BASE_URL + '/categorization/'
            print(" POST", url)
            print(' Training...')

            res = requests.post(url,
                                json={'index': seed_index,
                                      'y': seed_y,
                                      'parent_id': parent_id,
                                      'method': method,  # one of "LinearSVC", "LogisticRegression", 'xgboost'
                                      }).json()

            mid = res['id']
            print("     => model id = {}".format(mid))
            print('    => Training scores: MAP = {average_precision:.3f}, ROC-AUC = {roc_auc:.3f}'.format(**res))

            print("\n3.b. Check the parameters used in the categorization model")
            url = BASE_URL + '/categorization/{}'.format(mid)
            print(" GET", url)
            res = requests.get(url).json()

            print('\n'.join(['     - {}: {}'.format(key, val) for key, val in res.items() \
                                                              if key not in ['index', 'y']]))

            print("\n3.c Categorize the complete dataset with this model")
            url = BASE_URL + '/categorization/{}/predict'.format(mid)
            print(" GET", url)
            res = requests.get(url).json()
            prediction = res['prediction']

            if method == "NearestNeighbor":
                df = pd.DataFrame({key: res[key] for key in res if key not in ['id', 'scores']})

            print("\n3.d Test categorization accuracy")
            print("         using {}".format(ground_truth_file))  
            url = BASE_URL + '/categorization/{}/test'.format(mid)
            print("POST", url)
            res = requests.post(url, json={'ground_truth_filename': ground_truth_file}).json()

            print('    => Test scores: MAP = {average_precision:.3f}, ROC-AUC = {roc_auc:.3f}'.format(**res))

        print('\n', df)

        # 4. Cleaning
        print("\n5.a Delete the extracted features (and LSI decomposition)")
        url = BASE_URL + '/feature-extraction/{}'.format(dsid)
        print(" DELETE", url)
        requests.delete(url)

**Total running time of the script:** ( 0 minutes  59.590 seconds)



.. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: REST_categorization.py <REST_categorization.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: REST_categorization.ipynb <REST_categorization.ipynb>`

.. rst-class:: sphx-glr-signature

    `Generated by Sphinx-Gallery <http://sphinx-gallery.readthedocs.io>`_
