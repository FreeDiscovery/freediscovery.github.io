

.. _sphx_glr_examples_REST_semantic_search.py:


Semantic Search Example [REST API]
----------------------------------

An example of Semantic Search





.. rst-class:: sphx-glr-script-out

 Out::

    0. Load the test dataset
     GET http://localhost:5001/api/v0/example-dataset/treclegal09_2k_subset

    1.a Load dataset and initalize feature extraction
     POST http://localhost:5001/api/v0/feature-extraction
       => received ['filenames', 'id']
       => dsid = fb9df08fbe954de6

    1.b Start feature extraction
     POST http://localhost:5001/api/v0/feature-extraction/fb9df08fbe954de6

    2. Calculate LSI
    POST http://localhost:5001/api/v0/lsi/
      => LSI model id = e270410d14a840a2
      => SVD decomposition with 100 dimensions explaining 99.79 % variabilty of the data

    3.a. Perform the semantic search
     POST http://localhost:5001/api/v0/search/
                 document_id  score
    internal_id                    
    2128             4528384  0.449
    450               202500  0.444
    1406             1976836  0.436
    516               266256  0.428
    694               481636  0.424
    146                21316  0.421
    2219             4923961  0.412
    881               776161  0.410
    1297             1682209  0.410
    83                  6889  0.409
    2089             4363921  0.409
    1936             3748096  0.409
    1130             1276900  0.406
    82                  6724  0.405
    1129             1274641  0.405
    2099             4405801  0.405
    821               674041  0.403
    880               774400  0.402
    1283             1646089  0.401
    858               736164  0.400
    2096             4393216  0.400
    1298             1684804  0.400
    882               777924  0.399
    1906             3632836  0.398
    95                  9025  0.397
    1448             2096704  0.396
    878               770884  0.396
    1295             1677025  0.396
    1254             1572516  0.395
    964               929296  0.395
    ...                  ...    ...
    158                24964  0.157
    1935             3744225  0.156
    552               304704  0.156
    218                47524  0.156
    345               119025  0.156
    2059             4239481  0.155
    2070             4284900  0.155
    361               130321  0.154
    128                16384  0.154
    1042             1085764  0.153
    1180             1392400  0.152
    1997             3988009  0.152
    2281             5202961  0.152
    2277             5184729  0.151
    353               124609  0.151
    45                  2025  0.151
    206                42436  0.150
    344               118336  0.149
    1041             1083681  0.149
    1820             3312400  0.149
    2053             4214809  0.149
    2169             4704561  0.148
    2273             5166529  0.147
    36                  1296  0.147
    205                42025  0.146
    57                  3249  0.145
    322               103684  0.143
    2257             5094049  0.140
    321               103041  0.138
    704               495616  0.136

    [2465 rows x 2 columns]
    0.448608513009

    4. Delete the extracted features
     DELETE http://localhost:5001/api/v0/feature-extraction/fb9df08fbe954de6




|


.. code-block:: python


    from __future__ import print_function

    import os.path
    import requests
    import pandas as pd

    pd.options.display.float_format = '{:,.3f}'.format
    pd.options.display.expand_frame_repr = False

    dataset_name = "treclegal09_2k_subset"     # see list of available datasets

    BASE_URL = "http://localhost:5001/api/v0"  # FreeDiscovery server URL

    if __name__ == '__main__':

        print(" 0. Load the test dataset")
        url = BASE_URL + '/example-dataset/{}'.format(dataset_name)
        print(" GET", url)
        input_ds = requests.get(url).json()

        # create a custom dataset definition for ingestion
        data_dir = input_ds['metadata']['data_dir']
        dataset_definition = [{'document_id': row['document_id'],
                               'file_path': os.path.join(data_dir, row['file_path'])} \
                                       for row in input_ds['dataset']]

        # 1. Feature extraction

        print("\n1.a Load dataset and initalize feature extraction")
        url = BASE_URL + '/feature-extraction'
        print(" POST", url)
        res = requests.post(url, json={'dataset_definition': dataset_definition,
                                       'use_hashing': True}).json()

        dsid = res['id']
        print("   => received {}".format(list(res.keys())))
        print("   => dsid = {}".format(dsid))

        print("\n1.b Start feature extraction")

        url = BASE_URL+'/feature-extraction/{}'.format(dsid)
        print(" POST", url)
        requests.post(url)

        # 3. Document categorization with LSI (used for Nearest Neighbors method)

        print("\n2. Calculate LSI")

        url = BASE_URL + '/lsi/'
        print("POST", url)

        n_components = 100
        res = requests.post(url,
                            json={'n_components': n_components,
                                  'parent_id': dsid
                                  }).json()

        lsi_id = res['id']
        print('  => LSI model id = {}'.format(lsi_id))
        print('  => SVD decomposition with {} dimensions explaining {:.2f} % variabilty of the data'.format(
                                n_components, res['explained_variance']*100))


        # 3. Semantic search

        print("\n3.a. Perform the semantic search")


        query = """There are some conflicts with the draft date, so we will probably need to
                    have it on a different date."""

        url = BASE_URL + '/search/'
        print(" POST", url)

        res = requests.post(url,
                            json={'parent_id': lsi_id,
                                  'query': query
                                  }).json()

        data = res['data']

        df = pd.DataFrame(data).set_index('internal_id')
        print(df)

        print(df.score.max())


        # 4. Cleaning
        print("\n4. Delete the extracted features")
        url = BASE_URL + '/feature-extraction/{}'.format(dsid)
        print(" DELETE", url)
        requests.delete(url)

**Total running time of the script:** ( 0 minutes  9.198 seconds)



.. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: REST_semantic_search.py <REST_semantic_search.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: REST_semantic_search.ipynb <REST_semantic_search.ipynb>`

.. rst-class:: sphx-glr-signature

    `Generated by Sphinx-Gallery <http://sphinx-gallery.readthedocs.io>`_
