

.. _sphx_glr_examples_REST_semantic_search.py:


Semantic Search Example [REST API]
----------------------------------

An example of Semantic Search





.. rst-class:: sphx-glr-script-out

 Out::

    0. Load the test dataset
     GET http://localhost:5001/api/v0/example-dataset/treclegal09_2k_subset

    1.a Load dataset and initalize feature extraction
     POST http://localhost:5001/api/v0/feature-extraction
       => received ['filenames', 'id']
       => dsid = 06d7a1ef043d4b62

    1.b Start feature extraction
     POST http://localhost:5001/api/v0/feature-extraction/06d7a1ef043d4b62

    2. Calculate LSI
    POST http://localhost:5001/api/v0/lsi/
      => LSI model id = f65669e817524565
      => SVD decomposition with 100 dimensions explaining 99.79 % variabilty of the data

    3.a. Perform the semantic search
     POST http://localhost:5001/api/v0/search/
                 document_id  score
    internal_id                    
    878               770884  0.440
    1295             1677025  0.440
    1936             3748096  0.439
    1130             1276900  0.435
    1129             1274641  0.434
    450               202500  0.433
    694               481636  0.428
    1406             1976836  0.424
    2128             4528384  0.410
    1906             3632836  0.406
    516               266256  0.405
    1911             3651921  0.397
    1108             1227664  0.396
    1489             2217121  0.393
    1284             1648656  0.392
    585               342225  0.390
    964               929296  0.388
    861               741321  0.388
    1883             3545689  0.388
    961               923521  0.387
    860               739600  0.387
    881               776161  0.385
    1297             1682209  0.385
    821               674041  0.382
    2089             4363921  0.382
    1882             3541924  0.381
    83                  6889  0.381
    1915             3667225  0.380
    1907             3636649  0.380
    1283             1646089  0.379
    ...                  ...    ...
    1935             3744225  0.155
    2117             4481689  0.155
    226                51076  0.155
    1171             1371241  0.155
    1988             3952144  0.155
    704               495616  0.155
    2257             5094049  0.154
    2137             4566769  0.154
    552               304704  0.153
    1180             1392400  0.153
    1997             3988009  0.153
    153                23409  0.153
    128                16384  0.152
    1042             1085764  0.152
    57                  3249  0.152
    321               103041  0.151
    2095             4389025  0.151
    2277             5184729  0.151
    158                24964  0.150
    344               118336  0.150
    206                42436  0.149
    93                  8649  0.149
    2059             4239481  0.149
    92                  8464  0.147
    45                  2025  0.147
    2169             4704561  0.147
    1820             3312400  0.146
    2273             5166529  0.146
    205                42025  0.145
    1041             1083681  0.145

    [2465 rows x 2 columns]
    0.439985479601

    4. Delete the extracted features
     DELETE http://localhost:5001/api/v0/feature-extraction/06d7a1ef043d4b62




|


.. code-block:: python


    from __future__ import print_function

    import os.path
    import requests
    import pandas as pd

    pd.options.display.float_format = '{:,.3f}'.format
    pd.options.display.expand_frame_repr = False

    dataset_name = "treclegal09_2k_subset"     # see list of available datasets

    BASE_URL = "http://localhost:5001/api/v0"  # FreeDiscovery server URL

    if __name__ == '__main__':

        print(" 0. Load the test dataset")
        url = BASE_URL + '/example-dataset/{}'.format(dataset_name)
        print(" GET", url)
        input_ds = requests.get(url).json()

        # create a custom dataset definition for ingestion
        data_dir = input_ds['metadata']['data_dir']
        dataset_definition = [{'document_id': row['document_id'],
                               'file_path': os.path.join(data_dir, row['file_path'])} \
                                       for row in input_ds['dataset']]

        # 1. Feature extraction

        print("\n1.a Load dataset and initalize feature extraction")
        url = BASE_URL + '/feature-extraction'
        print(" POST", url)
        res = requests.post(url, json={'dataset_definition': dataset_definition,
                                       'use_hashing': True}).json()

        dsid = res['id']
        print("   => received {}".format(list(res.keys())))
        print("   => dsid = {}".format(dsid))

        print("\n1.b Start feature extraction")

        url = BASE_URL+'/feature-extraction/{}'.format(dsid)
        print(" POST", url)
        requests.post(url)

        # 3. Document categorization with LSI (used for Nearest Neighbors method)

        print("\n2. Calculate LSI")

        url = BASE_URL + '/lsi/'
        print("POST", url)

        n_components = 100
        res = requests.post(url,
                            json={'n_components': n_components,
                                  'parent_id': dsid
                                  }).json()

        lsi_id = res['id']
        print('  => LSI model id = {}'.format(lsi_id))
        print('  => SVD decomposition with {} dimensions explaining {:.2f} % variabilty of the data'.format(
                                n_components, res['explained_variance']*100))


        # 3. Semantic search

        print("\n3.a. Perform the semantic search")


        query = """There are some conflicts with the draft date, so we will probably need to
                    have it on a different date."""

        url = BASE_URL + '/search/'
        print(" POST", url)

        res = requests.post(url,
                            json={'parent_id': lsi_id,
                                  'query': query
                                  }).json()

        data = res['data']

        df = pd.DataFrame(data).set_index('internal_id')
        print(df)

        print(df.score.max())


        # 4. Cleaning
        print("\n4. Delete the extracted features")
        url = BASE_URL + '/feature-extraction/{}'.format(dsid)
        print(" DELETE", url)
        requests.delete(url)

**Total running time of the script:** ( 0 minutes  10.863 seconds)



.. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: REST_semantic_search.py <REST_semantic_search.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: REST_semantic_search.ipynb <REST_semantic_search.ipynb>`

.. rst-class:: sphx-glr-signature

    `Generated by Sphinx-Gallery <http://sphinx-gallery.readthedocs.io>`_
