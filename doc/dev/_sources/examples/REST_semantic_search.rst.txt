

.. _sphx_glr_examples_REST_semantic_search.py:


Semantic Search Example [REST API]
----------------------------------

An example of Semantic Search





.. rst-class:: sphx-glr-script-out

 Out::

    0. Load the test dataset
     GET http://localhost:5001/api/v0/example-dataset/treclegal09_2k_subset

    1.a Load dataset and initalize feature extraction
     POST http://localhost:5001/api/v0/feature-extraction
       => received ['id']
       => dsid = b043d8dc42c04faf

    1.b Start feature extraction
     POST http://localhost:5001/api/v0/feature-extraction/b043d8dc42c04faf

    2. Calculate LSI
    POST http://localhost:5001/api/v0/lsi/
      => LSI model id = d590e72b213e48c5
      => SVD decomposition with 100 dimensions explaining 76.42 % variabilty of the data

    3.a. Perform the semantic search
     POST http://localhost:5001/api/v0/search/
                 score
    document_id       
    4528384      0.435
    21316        0.397
    4116841      0.371
    1572516      0.370
    2152089      0.368
    3545689      0.356
    202500       0.355
    674041       0.350
    1454436      0.349
    1976836      0.348
    68644        0.320
    69169        0.313
    14641        0.311
    4923961      0.311
    4464769      0.302
    1550025      0.295
    648025       0.295
    649636       0.293
    1274641      0.293
    1276900      0.292
    3748096      0.290
    525625       0.285
    21609        0.273
    3651921      0.271
    102400       0.271
    4532641      0.270
    266256       0.270
    2856100      0.267
    1227664      0.265
    2217121      0.262
    ...            ...
    4410000     -0.211
    906304      -0.212
    2226064     -0.213
    950625      -0.214
    1607824     -0.216
    228484      -0.216
    3312400     -0.218
    114921      -0.219
    1085764     -0.220
    708964      -0.220
    1610361     -0.220
    952576      -0.220
    4293184     -0.222
    1083681     -0.224
    1311025     -0.225
    908209      -0.226
    1296        -0.227
    1288225     -0.234
    3751969     -0.234
    243049      -0.234
    872356      -0.241
    2050624     -0.241
    4214809     -0.250
    1510441     -0.259
    614656      -0.259
    5143824     -0.263
    2268036     -0.297
    495616      -0.299
    1763584     -0.302
    2271049     -0.313

    [2465 rows x 1 columns]
    0.435409689487

    4. Delete the extracted features
     DELETE http://localhost:5001/api/v0/feature-extraction/b043d8dc42c04faf




|


.. code-block:: python


    from __future__ import print_function

    import os.path
    import requests
    import pandas as pd

    pd.options.display.float_format = '{:,.3f}'.format
    pd.options.display.expand_frame_repr = False

    dataset_name = "treclegal09_2k_subset"     # see list of available datasets

    BASE_URL = "http://localhost:5001/api/v0"  # FreeDiscovery server URL

    if __name__ == '__main__':

        print(" 0. Load the test dataset")
        url = BASE_URL + '/example-dataset/{}'.format(dataset_name)
        print(" GET", url)
        input_ds = requests.get(url).json()

        # create a custom dataset definition for ingestion
        data_dir = input_ds['metadata']['data_dir']
        dataset_definition = [{'document_id': row['document_id'],
                               'file_path': os.path.join(data_dir, row['file_path'])}
                              for row in input_ds['dataset']]

        # 1. Feature extraction

        print("\n1.a Load dataset and initalize feature extraction")
        url = BASE_URL + '/feature-extraction'
        print(" POST", url)
        res = requests.post(url).json()

        dsid = res['id']
        print("   => received {}".format(list(res.keys())))
        print("   => dsid = {}".format(dsid))

        print("\n1.b Start feature extraction")

        url = BASE_URL+'/feature-extraction/{}'.format(dsid)
        print(" POST", url)
        requests.post(url, json={'dataset_definition': dataset_definition})

        # 3. Document categorization with LSI (used for Nearest Neighbors method)

        print("\n2. Calculate LSI")

        url = BASE_URL + '/lsi/'
        print("POST", url)

        n_components = 100
        res = requests.post(url,
                            json={'n_components': n_components,
                                  'parent_id': dsid
                                  }).json()

        lsi_id = res['id']
        print('  => LSI model id = {}'.format(lsi_id))
        print('  => SVD decomposition with {} dimensions explaining {:.2f} % variabilty of the data'.format(
                                n_components, res['explained_variance']*100))


        # 3. Semantic search

        print("\n3.a. Perform the semantic search")


        query = """There are some conflicts with the draft date, so we will probably need to
                    have it on a different date."""

        url = BASE_URL + '/search/'
        print(" POST", url)

        res = requests.post(url,
                            json={'parent_id': lsi_id,
                                  'query': query
                                  }).json()

        data = res['data']

        df = pd.DataFrame(data).set_index('document_id')
        print(df)

        print(df.score.max())


        # 4. Cleaning
        print("\n4. Delete the extracted features")
        url = BASE_URL + '/feature-extraction/{}'.format(dsid)
        print(" DELETE", url)
        requests.delete(url)

**Total running time of the script:** ( 0 minutes  4.369 seconds)



.. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: REST_semantic_search.py <REST_semantic_search.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: REST_semantic_search.ipynb <REST_semantic_search.ipynb>`

.. rst-class:: sphx-glr-signature

    `Generated by Sphinx-Gallery <http://sphinx-gallery.readthedocs.io>`_
