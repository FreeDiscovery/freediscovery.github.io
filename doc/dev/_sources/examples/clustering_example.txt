

.. _sphx_glr_examples_clustering_example.py:


Clustering Example [REST API]
-----------------------------

Cluster documents into clusters





.. rst-class:: sphx-glr-script-out

 Out::

    0. Load the test dataset
     POST http://localhost:5001/api/v0/datasets/treclegal09_2k_subset

    1.a Load dataset and initalize feature extraction
     POST http://localhost:5001/api/v0/feature-extraction
       => received [u'id', u'filenames']
       => dsid = 0284887d7e30489a89b809adcc86c161

    1.b Run feature extraction
     POST http://localhost:5001/api/v0/feature-extraction/0284887d7e30489a89b809adcc86c161

    1.d. check the parameters of the extracted features
     GET http://localhost:5001/api/v0/feature-extraction/0284887d7e30489a89b809adcc86c161
         - binary: False
         - n_jobs: -1
         - stop_words: english
         - use_hashing: False
         - min_df: 0.0
         - n_samples: 2465
         - analyzer: word
         - ngram_range: [1, 1]
         - max_df: 1.0
         - chunk_size: 2000
         - use_idf: True
         - data_dir: ../freediscovery_shared/treclegal09_2k_subset/data
         - sublinear_tf: True
         - n_samples_processed: 2465
         - n_features: 30001
         - norm: l2

    2.a. Document clustering (LSI + K-means)
     POST http://localhost:5001/api/v0/clustering/k-mean/
         => model id = 0a5cf63b617247bea9b56b9abfde1a67

    2.b. Computing cluster labels
     POST http://localhost:5001/api/v0/clustering/k-mean/0a5cf63b617247bea9b56b9abfde1a67
        .. computed in 3.8s
       N_documents                                      cluster_names
    2          580  [u'recipients', u'administrative', u'group', u...
    1          496  [u'normal', u'tenet', u'test', u'gmt', u'sensi...
    3          393  [u'ect', u'hou', u'shackleton', u'test', u'nem...
    7          303  [u'enron', u'energy', u'trade', u'power', u'se...
    5          234  [u'enron', u'dasovich', u'normal', u'alias', u...
    4          124  [u'shall', u'party', u'agreement', u'transacti...
    6          111  [u'enron_development', u'ect', u'shackleton', ...
    8          103  [u'sanders', u'normal', u'nov', u'ect', u'subj...
    0           64  [u'migration', u'outlook', u'team', u'normal',...
    9           57  [u'rewrite', u'server', u'address', u'smtp', u...

    2.a. Document clustering (LSI + Ward HC)
     POST http://localhost:5001/api/v0/clustering/ward_hc/
         => model id = 44a57159b4c64a59b1c330d1ae40f753

    2.b. Computing cluster labels
    POST http://localhost:5001/api/v0/clustering/ward_hc/44a57159b4c64a59b1c330d1ae40f753
        .. computed in 4.4s
       N_documents                                      cluster_names
    0          511  [u'enron', u'nemec', u'ect', u'energy', u'trad...
    2          447  [u'normal', u'tenet', u'test', u'gmt', u'sensi...
    4          437  [u'recipients', u'administrative', u'group', u...
    1          421  [u'shackleton', u'enron_development', u'ect', ...
    8          168  [u'ect', u'hou', u'group', u'recipients', u'ad...
    3          142  [u'shall', u'party', u'agreement', u'transacti...
    9          125  [u'tana', u'jones', u'enron', u'normal', u'tra...
    7           95  [u'sanders', u'normal', u'nov', u'sensitivity'...
    5           63  [u'migration', u'outlook', u'team', u'normal',...
    6           56  [u'rewrite', u'server', u'address', u'smtp', u...




|


.. code-block:: python


    import numpy as np
    import pandas as pd
    from time import time
    import requests

    pd.options.display.float_format = '{:,.3f}'.format


    def repr_clustering(labels, terms):
        out = []
        for ridx, row in enumerate(terms):
            out.append({'cluster_names': row, 'N_documents': (labels == ridx).sum()})
        out = pd.DataFrame(out).sort_values('N_documents', ascending=False)
        return out

    dataset_name = "treclegal09_2k_subset"     # see list of available datasets

    BASE_URL = "http://localhost:5001/api/v0"  # FreeDiscovery server URL

    print(" 0. Load the test dataset")
    url = BASE_URL + '/datasets/{}'.format(dataset_name)
    print(" POST", url)
    res = requests.get(url)
    res = res.json()

    # To use a custom dataset, simply specify the following variables
    data_dir = res['data_dir']

    # # 1. Feature extraction (non hashed)

    print("\n1.a Load dataset and initalize feature extraction")
    url = BASE_URL + '/feature-extraction'
    print(" POST", url)
    fe_opts = {'data_dir': data_dir,
               'stop_words': 'english', 'chunk_size': 2000, 'n_jobs': -1,
               'use_idf': 1, 'sublinear_tf': 1, 'binary': 0, 'n_features': 30001,
               'analyzer': 'word', 'ngram_range': (1, 1), "norm": "l2",
               'use_hashing': False  # hashing should be disabled for clustering
               }
    res = requests.post(url, json=fe_opts)

    dsid = res.json()['id']
    print("   => received {}".format(list(res.json().keys())))
    print("   => dsid = {}".format(dsid))


    print("\n1.b Run feature extraction")
    # progress status is available for the hashed version only
    url = BASE_URL+'/feature-extraction/{}'.format(dsid)
    print(" POST", url)
    res = requests.post(url)

    print("\n1.d. check the parameters of the extracted features")
    url = BASE_URL + '/feature-extraction/{}'.format(dsid)
    print(' GET', url)
    res = requests.get(url)

    data = res.json()
    print('\n'.join(['     - {}: {}'.format(key, val) for key, val in data.items() \
                                                      if "filenames" not in key]))


    # # 2. Document Clustering (LSI + K-Means)

    print("\n2.a. Document clustering (LSI + K-means)")

    url = BASE_URL + '/clustering/k-mean/'
    print(" POST", url)
    t0 = time()
    res = requests.post(url,
                        json={'dataset_id': dsid,
                              'n_clusters': 10,
                              'lsi_components': 50
                              })

    data = res.json()
    mid = data['id']
    print("     => model id = {}".format(mid))

    print("\n2.b. Computing cluster labels")
    url = BASE_URL + '/clustering/k-mean/{}'.format(mid)
    print(" POST", url)
    res = requests.get(url,
                       json={'n_top_words': 6
                             })
    data = res.json()
    t1 = time()

    print('    .. computed in {:.1f}s'.format(t1 - t0))
    print(repr_clustering(np.array(data['labels']), data['cluster_terms']))


    # # 3. Document Clustering (LSI + Ward Hierarchical Clustering)

    print("\n2.a. Document clustering (LSI + Ward HC)")

    url = BASE_URL + '/clustering/ward_hc/'
    print(" POST", url)
    t0 = time()
    res = requests.post(url,
                        json={'dataset_id': dsid,
                              'n_clusters': 10,
                              'lsi_components': 50,
                              'n_neighbors': 5  # this is the connectivity constraint
                              })

    data = res.json()
    mid = data['id']
    print("     => model id = {}".format(mid))

    print("\n2.b. Computing cluster labels")
    url = BASE_URL + '/clustering/ward_hc/{}'.format(mid)
    print("POST", url)
    res = requests.get(url,
                       json={'n_top_words': 6
                             })
    data = res.json()
    t1 = time()

    print('    .. computed in {:.1f}s'.format(t1 - t0))
    print(repr_clustering(np.array(data['labels']), data['cluster_terms']))

**Total running time of the script:** ( 0 minutes  10.719 seconds)



.. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: clustering_example.py <clustering_example.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: clustering_example.ipynb <clustering_example.ipynb>`

.. rst-class:: sphx-glr-signature

    `Generated by Sphinx-Gallery <http://sphinx-gallery.readthedocs.io>`_
