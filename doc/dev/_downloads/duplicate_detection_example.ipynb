{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nDuplicate Detection Example [REST API]\n--------------------------------------\n\nFind near-duplicates in a text collection\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from __future__ import print_function\n\nfrom time import time\nimport sys\nimport platform\n\nimport numpy as np\nimport pandas as pd\nimport requests\n\npd.options.display.float_format = '{:,.3f}'.format\n\n\ndataset_name = \"treclegal09_2k_subset\"     # see list of available datasets\n\nBASE_URL = \"http://localhost:5001/api/v0\"  # FreeDiscovery server URL\n\nprint(\" 0. Load the test dataset\")\nurl = BASE_URL + '/datasets/{}'.format(dataset_name)\nprint(\" POST\", url)\nres = requests.get(url)\nres = res.json()\n\n# To use a custom dataset, simply specify the following variables\ndata_dir = res['data_dir']\n# # 1. Feature extraction (non hashed)\n\nprint(\"\\n1.a Load dataset and initalize feature extraction\")\nurl = BASE_URL + '/feature-extraction'\nprint(\" POST\", url)\nfe_opts = {'data_dir': data_dir,\n           'stop_words': 'english', 'chunk_size': 2000, 'n_jobs': -1,\n           'use_idf': 1, 'sublinear_tf': 0, 'binary': 0, 'n_features': 30001,\n           'analyzer': 'word', 'ngram_range': (1, 1), \"norm\": \"l2\",\n           'use_hashing': False,  # hashing should be disabled for clustering\n           #'min_df': 0.2, 'max_df': 0.8\n          }\nres = requests.post(url, json=fe_opts)\n\ndsid = res.json()['id']\nprint(\"   => received {}\".format(list(res.json().keys())))\nprint(\"   => dsid = {}\".format(dsid))\n\n\nprint(\"\\n1.b Run feature extraction\")\n# progress status is available for the hashed version only\nurl = BASE_URL+'/feature-extraction/{}'.format(dsid)\nprint(\" POST\", url)\nres = requests.post(url)\n\nprint(\"\\n1.d. check the parameters of the extracted features\")\nurl = BASE_URL + '/feature-extraction/{}'.format(dsid)\nprint(' GET', url)\nres = requests.get(url)\n\ndata = res.json()\nprint('\\n'.join(['     - {}: {}'.format(key, val) for key, val in data.items() \\\n                                                  if \"filenames\" not in key]))\n\n\n# # 2. Duplicate detection by cosine similarity (DBSCAN)\n\nprint(\"\\n2. Duplicate detection by cosine similarity (DBSCAN)\")\n\nurl = BASE_URL + '/clustering/dbscan/'\nprint(\" POST\", url)\nt0 = time()\nres = requests.post(url,\n        json={'dataset_id': dsid,\n              'lsi_components': 100,\n              'eps': 0.1,            # threashold for 2 documents considered to be duplicates\n              'n_max_samples': 2\n              }) \n\ndata = res.json()\nmid  = data['id']\nprint(\"     => model id = {}\".format(mid))\n\nurl = BASE_URL + '/clustering/dbscan/{}'.format(mid)\nprint(\" POST\", url)\nres = requests.get(url,\n        json={'n_top_words': 0, # don't compute cluster labels\n              }) \ndata = res.json()\nt1 = time()\n\nprint('    .. computed in {:.1f}s'.format(t1 - t0))\n\nlabels_ = data['labels']\n\nprint('Found {} duplicates / {}'.format(len(labels_) - len(np.unique(labels_)), len(labels_)))\n\nprint(\"\\n3. Duplicate detection by I-Match\")\n\nurl = BASE_URL + '/duplicate-detection/'\nprint(\" POST\", url)\nt0 = time()\nres = requests.post(url,\n        json={'dataset_id': dsid,\n              'method': 'i-match',\n              }) \n\ndata = res.json()\nmid  = data['id']\nprint(\"     => model id = {}\".format(mid))\n\nprint('    .. computed in {:.1f}s'.format(time() - t0))\n\n\nurl = BASE_URL + '/duplicate-detection/{}'.format(mid)\nprint(\"GET\", url)\nt0 = time()\nres = requests.get(url,\n        json={'n_rand_lexicons': 10,\n              'rand_lexicon_ratio': 0.9}) \ndata = res.json()\nt1 = time()\nprint('    .. computed in {:.1f}s'.format(time() - t0))\n\nlabels_ = data['cluster_id']\n\nprint('Found {} duplicates / {}'.format(len(labels_) - len(np.unique(labels_)), len(labels_)))\n\n\nif platform.system() == 'Windows':\n    print('Simhash-py is currently not installed on Windows')\n    sys.exit()\n\nprint(\"\\n3. Duplicate detection by Simhash\")\n\nurl = BASE_URL + '/duplicate-detection/'\nprint(\" POST\", url)\nt0 = time()\nres = requests.post(url,\n        json={'dataset_id': dsid,\n              'method': 'simhash',\n              }) \n\ndata = res.json()\nmid  = data['id']\nprint(\"     => model id = {}\".format(mid))\n\nprint('    .. computed in {:.1f}s'.format(time() - t0))\n\n\n\nurl = BASE_URL + '/duplicate-detection/{}'.format(mid)\nprint(\" GET\", url)\nt0 = time()\nres = requests.get(url,\n        json={'distance': 1 }) \ndata = res.json()\nprint('    .. computed in {:.1f}s'.format(time() - t0))\n\nlabels_ = data['cluster_id']\n\nprint('Found {} duplicates / {}'.format(len(labels_) - len(np.unique(labels_)), len(labels_)))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.11", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}