{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nDuplicate Detection\n===================\n\nFind near-duplicates in a text collection\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n\nfrom time import time\n\nimport pandas as pd\nimport requests\n\npd.options.display.float_format = '{:,.3f}'.format\n\n\ndataset_name = \"treclegal09_2k_subset\"     # see list of available datasets\n\nBASE_URL = \"http://localhost:5001/api/v0\"  # FreeDiscovery server URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "0. Load the test dataset\n------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = BASE_URL + '/example-dataset/{}'.format(dataset_name)\nprint(\" GET\", url)\ninput_ds = requests.get(url).json()\n\n\n# To use a custom dataset, simply specify the following variables\ndata_dir = input_ds['metadata']['data_dir']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Feature extraction (non hashed)\n----------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n1.a Load dataset and initalize feature extraction\")\nurl = BASE_URL + '/feature-extraction'\nprint(\" POST\", url)\n\nfe_opts = {'weighting': 'ntc',\n           'n_features': 30001,\n           'min_df': 4, 'max_df': 0.75\n           }\nres = requests.post(url, json=fe_opts)\n\ndsid = res.json()['id']\nprint(\"   => received {}\".format(list(res.json().keys())))\nprint(\"   => dsid = {}\".format(dsid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.b Run feature extraction\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = BASE_URL+'/feature-extraction/{}'.format(dsid)\nprint(\" POST\", url)\nres = requests.post(url, json={\"data_dir\": data_dir})\n\nprint(\"\\n1.d. check the parameters of the extracted features\")\nurl = BASE_URL + '/feature-extraction/{}'.format(dsid)\nprint(' GET', url)\nres = requests.get(url)\n\ndata = res.json()\nprint('\\n'.join(['     - {}: {}'.format(key, val)\n      for key, val in data.items() if \"filenames\" not in key]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Compute LSI\n--------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = BASE_URL + '/lsi/'\nprint(\"POST\", url)\n\nn_components = 100\nres = requests.post(url,\n                    json={'n_components': n_components,\n                          'parent_id': dsid\n                          }).json()\n\nlsi_id = res['id']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Near Duplicates detection by cosine similarity (DBSCAN)\n----------------------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = BASE_URL + '/clustering/dbscan/'\nprint(\" POST\", url)\nt0 = time()\nres = requests.post(url,\n                    json={'parent_id': lsi_id,\n                          'min_similarity': 0.90,\n                          'n_max_samples': 2\n                          }).json()\n\nmid = res['id']\nprint(\"     => model id = {}\".format(mid))\n\nurl = BASE_URL + '/clustering/dbscan/{}'.format(mid)\nprint(\" GET\", url)\n# don't compute cluster labels\nres = requests.get(url, json={'n_top_words': 0}).json()\nt1 = time()\n\nprint('    .. computed in {:.1f}s'.format(t1 - t0))\n\ndata = res['data']\nprint('Found {} duplicates / {}'\n      .format(sum([len(row['documents'])\n                   for row in data if len(row['documents']) > 1]),\n              len(input_ds['dataset'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Near Duplicates Detection using I-Match\n------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = BASE_URL + '/duplicate-detection/'\nprint(\" POST\", url)\nt0 = time()\nres = requests.post(url, json={'parent_id': dsid,\n                               'method': 'i-match'})\n\ndata = res.json()\nmid = data['id']\nprint(\"     => model id = {}\".format(mid))\n\nprint('    .. computed in {:.1f}s'.format(time() - t0))\n\n\nurl = BASE_URL + '/duplicate-detection/{}'.format(mid)\nprint(\" GET\", url)\nt0 = time()\nres = requests.get(url, json={'n_rand_lexicons': 10,\n                              'rand_lexicon_ratio': 0.9}).json()\nt1 = time()\nprint('    .. computed in {:.1f}s'.format(time() - t0))\n\ndata = res['data']\n\nprint('Found {} duplicates / {}'\n      .format(sum([len(row['documents']) for row in data]),\n              len(input_ds['dataset'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Duplicate detection by Simhash\n---------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    import simhash\n    url = BASE_URL + '/duplicate-detection/'\n    print(\" POST\", url)\n    t0 = time()\n    res = requests.post(url, json={'parent_id': dsid,\n                                   'method': 'simhash'})\n\n    data = res.json()\n    mid = data['id']\n    print(\"     => model id = {}\".format(mid))\n\n    print('    .. computed in {:.1f}s'.format(time() - t0))\n\n    url = BASE_URL + '/duplicate-detection/{}'.format(mid)\n    print(\" GET\", url)\n    t0 = time()\n    res = requests.get(url, json={'distance': 1})\n    data = res.json()\n    print('    .. computed in {:.1f}s'.format(time() - t0))\n\n    data = data['data']\n\n    print('Found {} duplicates / {}'\n          .format(sum([len(row['documents']) for row in data]),\n                  len(input_ds['dataset'])))\nexcept ImportError:\n    print(\"simhash is not installed or not supported \"\n          \" (e.g. on Windows)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4 Delete the extracted features\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = BASE_URL + '/feature-extraction/{}'.format(dsid)\nrequests.delete(url)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}