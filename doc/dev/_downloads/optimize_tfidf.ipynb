{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nOptimizing TF-IDF schemes\n=========================\n\nAn example of optimizing TF-IDF weighting schemes using\n5 fold cross-validation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n\nimport os\nfrom itertools import product\n\nimport numpy as np\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nfrom freediscovery.feature_weighting import SmartTfidfTransformer\n\nrng = np.random.RandomState(34)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load and vectorize 2 classes from the 20 newsgroup dataset,\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "newsgroups = fetch_20newsgroups(subset='train',\n                                categories=['sci.space', 'comp.graphics'])\nvectorizer = CountVectorizer(stop_words='english')\nX = vectorizer.fit_transform(newsgroups.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "then compute baseline categorization performance using Logistic Regression\nand the TF-IDF transfomer from scikit-learn\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_tfidf = TfidfTransformer().fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, newsgroups.target,\n                                                    random_state=rng)\n\npipe = Pipeline(steps=[('tfidf', TfidfTransformer()),\n                       ('logisticregression', LogisticRegression())])\n\npipe.fit(X_train, y_train)\nprint('Baseline TF-IDF categorization accuracy: {:.3f}'\n      .format(pipe.score(X_test, y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we search, using 5 fold cross-validation, for the best TF-IDF weighting\nscheme among the 80+ combinations supported by\n:class:`~freediscovery.feature_weighting.SmartTfidfTransformer`. Two\nhyper-parameters are worth optimizing in this case,\n\n* ``weighting`` parameter that defines the TF-IDF weighting (see the\n  `tfidf_section` user manual section for more details)\n* ``norm_alpha`` is the \u03b1 parameter in the pivoted normalization\n  when ``weighting==\"???p\"``.\n\nTo reduce the search parameter space in this example, we also can exclude\nthe case when either the term weighting, feature weighing or normalization is\nnot used as it expected to yield worse than baseline performance. We also\nexclude the non smoothed IDF weightings (``?t?``, ``?p?``) since thay return\nNaNs when some of the document frequency is 0 (which will be the case\nduring cross-validation). Finally, by noticing\nthat the case ``xxxp`` with  ``norm_alpha=1.0`` corresponds to the weighing\n``xxx`` (i.e. with pivoted normalization disabled) we can reduce the search\nspace even further.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe = Pipeline(steps=[('tfidf', SmartTfidfTransformer()),\n                       ('logisticregression', LogisticRegression())])\n\nparam_grid = {'tfidf__weighting': [\"\".join(el) + 'p'\n                                   for el in product('labLd', 'sd',\n                                                     \"clu\")],\n              'tfidf__norm_alpha': np.linspace(0, 1, 10)}\n\npipe_cv = GridSearchCV(pipe,\n                       param_grid=param_grid,\n                       verbose=1,\n                       n_jobs=(1 if os.name == 'nt' else -1),\n                       cv=5)\npipe_cv.fit(X_train, y_train)\nprint('Best CV params: weighting={weighting}, norm_alpha={norm_alpha:.3f} '\n      .format(**pipe_cv.best_estimator_.steps[0][1].get_params()))\nprint('Best TF-IDF categorization accuracy: {:.3f}'\n      .format(pipe_cv.score(X_test, y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, by tuning TF-IDF weighting scheme with pivoted\nnormalization, we obtain a categorization accuracy score of 0.99 as compared\nto a baseline TF-IDF score of 0.973. It is also interesting to notice that\nthe best weighting hyper-parameter in this case is ``lnup`` which\ncorresponds to the \"unique pivoted normalization\" case proposed by\nSinghal *et al.* (1996), although with a different \u03b1 value.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}