{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nCategorization\n==============\n\nAn example to illustrate binary categorizaiton with FreeDiscovery\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n\nimport os.path\nimport requests\nimport pandas as pd\n\npd.options.display.float_format = '{:,.3f}'.format\npd.options.display.expand_frame_repr = False\n\ndataset_name = \"treclegal09_2k_subset\"     # see list of available datasets\n\nBASE_URL = \"http://localhost:5001/api/v0\"  # FreeDiscovery server URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "0. Load the example dataset\n---------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = BASE_URL + '/example-dataset/{}'.format(dataset_name)\nprint(\" GET\", url)\ninput_ds = requests.get(url).json()\n\n\ndata_dir = input_ds['metadata']['data_dir']\ndataset_definition = [{'document_id': row['document_id'],\n                       'file_path': os.path.join(data_dir, row['file_path'])}\n                      for row in input_ds['dataset']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Feature extraction\n---------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n1.a Load dataset and initalize feature extraction\")\nurl = BASE_URL + '/feature-extraction'\nprint(\" POST\", url)\nres = requests.post(url).json()\n\ndsid = res['id']\nprint(\"   => received {}\".format(list(res.keys())))\nprint(\"   => dsid = {}\".format(dsid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.b Start feature extraction (in the background)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = BASE_URL+'/feature-extraction/{}'.format(dsid)\nprint(\" POST\", url)\nres = requests.post(url, json={'dataset_definition': dataset_definition})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Calculate Latent Semantic Indexing\n-------------------------------------\n(used by Nearest Neighbors method)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = BASE_URL + '/lsi/'\nprint(\"POST\", url)\n\nn_components = 100\nres = requests.post(url,\n                    json={'n_components': n_components,\n                          'parent_id': dsid\n                          }).json()\n\nlsi_id = res['id']\nprint('  => LSI model id = {}'.format(lsi_id))\nprint(('  => SVD decomposition with {} dimensions explaining '\n       '{:.2f} % variabilty of the data')\n      .format(n_components, res['explained_variance']*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Document categorization\n--------------------------\n3.a. Train the categorization model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"   {} positive, {} negative files\".format(\n      pd.DataFrame(input_ds['training_set'])\n        .groupby('category').count()['document_id'], 0))\n\nfor method, use_lsi in [('LinearSVC', False),\n                        ('NearestNeighbor', True)]:\n\n    print('='*80, '\\n', ' '*10,\n          method, \" + LSI\" if use_lsi else ' ', '\\n', '='*80)\n    if use_lsi:\n        # Categorization with the previously created LSI model\n        parent_id = lsi_id\n    else:\n        # Categorization with original text features\n        parent_id = dsid\n\n    url = BASE_URL + '/categorization/'\n    print(\" POST\", url)\n    print(' Training...')\n\n    res = requests.post(url,\n                        json={'parent_id': parent_id,\n                              'data': input_ds['training_set'],\n                              'method': method,\n                              'training_scores': True\n                              }).json()\n\n    mid = res['id']\n    print(\"     => model id = {}\".format(mid))\n    print((\"    => Training scores: MAP = {average_precision:.3f}, \"\n           \"ROC-AUC = {roc_auc:.3f}, recall @20%: {recall_at_20p:.3f} \")\n          .format(**res['training_scores']))\n\n    print(\"\\n3.b. Check the parameters used in the categorization model\")\n    url = BASE_URL + '/categorization/{}'.format(mid)\n    print(\" GET\", url)\n    res = requests.get(url).json()\n\n    print('\\n'.join(['     - {}: {}'.format(key, val)\n          for key, val in res.items() if key not in ['index', 'category']]))\n\n    print(\"\\n3.c Categorize the complete dataset with this model\")\n    url = BASE_URL + '/categorization/{}/predict'.format(mid)\n    print(\" GET\", url)\n    res = requests.get(url, json={'subset': 'test'}).json()\n\n    data = []\n    for row in res['data']:\n        nrow = {'document_id': row['document_id'],\n                'category': row['scores'][0]['category'],\n                'score': row['scores'][0]['score']}\n        if method == 'NearestNeighbor':\n            nrow['nearest_document_id'] = row['scores'][0]['document_id']\n        data.append(nrow)\n\n    df = pd.DataFrame(data).set_index('document_id')\n    print(df)\n\n    print(\"\\n3.d Compute the categorization scores\")\n    url = BASE_URL + '/metrics/categorization'\n    print(\" GET\", url)\n    res = requests.post(url, json={'y_true': input_ds['dataset'],\n                                   'y_pred': res['data']}).json()\n\n    print((\"    => Test scores: MAP = {average_precision:.3f}, \"\n           \"ROC-AUC = {roc_auc:.3f}, recall @20%: {recall_at_20p:.3f} \")\n          .format(**res))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5 Delete the extracted features (and LSI decomposition)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = BASE_URL + '/feature-extraction/{}'.format(dsid)\nrequests.delete(url)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}