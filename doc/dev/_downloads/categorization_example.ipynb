{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nBinary Categorization Example\n-------------------------------\n\nThis example should be run in a Jupyter Notebook (cf. \"Examples\" section in FreeDiscovery Documentation for more detailed information)\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom time import time, sleep\nimport os\nfrom multiprocessing import Process\nimport requests\nimport pandas as pd\n\n\ndef _parent_dir(path, n=0):\n    path = os.path.abspath(path)\n    if n == 0:\n        return path\n    else:\n        return os.path.dirname(_parent_dir(path, n=n-1))\n\ndef _print_url(op, url):\n    print(' '*1, op, url) \n\nuse_docker = False\n\ndataset_name = \"treclegal09_2k_subset\"\n\nif use_docker:\n    data_dir = \"/freediscovery_shared/{}\".format(dataset_name)\nelse:\n    data_dir = \"../freediscovery_shared/{}\".format(dataset_name)\nrel_data_dir = os.path.abspath(\"../../freediscovery_shared/{}\".format(dataset_name)) # relative path between this file and the FreeDiscovery source folder\n\nBASE_URL = \"http://localhost:5001/api/v0\"  # FreeDiscovery server URL\n\n# 0. Load relevant and non relevant seed file list\n\nwith open(os.path.join(rel_data_dir,'seed_relevant.txt'), 'rt') as fh:\n    relevant_files = [el.strip() for el in fh.readlines()]\n\nwith open(os.path.join(rel_data_dir,'seed_non_relevant.txt'), 'rt') as fh:\n    non_relevant_files = [el.strip() for el in fh.readlines()]\n\n# Load ground truth file\nif use_docker:\n    gtfile = os.path.join(data_dir, \"ground_truth_file.txt\")  \nelse:\n    gtfile = os.path.join(rel_data_dir, \"ground_truth_file.txt\") \n\n\n# 1. Feature extraction\n\nprint(\"\\n1.a Load dataset and initalize feature extraction\")\nurl = BASE_URL + '/feature-extraction'\n_print_url(\"POST\", url)\nfe_opts = {'data_dir': os.path.join(data_dir, 'data'),\n           'stop_words': 'None', 'chunk_size': 2000, 'n_jobs': -1,\n           'use_idf': 1, 'sublinear_tf': 1, 'binary': 0, 'n_features': 50001,\n           'analyzer': 'word', 'ngram_range': (1, 1), \"norm\": \"l2\"\n          }\nres = requests.post(url, json=fe_opts)\n\ndsid = res.json()['id']\nprint(\"   => received {}\".format(list(res.json().keys())))\nprint(\"   => dsid = {}\".format(dsid))\n\nprint(\"\\n1.b Start feature extraction (in the background)\")\n\n# Make this call in a background process (there should be a better way of doing it)\nurl = BASE_URL+'/feature-extraction/{}'.format(dsid)\n_print_url(\"POST\", url)\np = Process(target=requests.post, args=(url,))\np.start()\nsleep(5.0) # wait a bit for the processing to start\n\nprint('\\n1.c Monitor feature extraction progress')\nurl = BASE_URL+'/feature-extraction/{}'.format(dsid)\n_print_url(\"GET\", url)\n\nt0 = time()\nwhile True:\n    res = requests.get(url)\n    if res.status_code == 520:\n        p.terminate()\n        raise ValueError('Processing did not start')\n    elif res.status_code == 200:\n        break # processing finished\n    data = res.json()\n    print('     ... {}k/{}k files processed in {:.1f} min'.format(\n                data['n_samples_processed']//1000, data['n_samples']//1000, (time() - t0)/60.))\n    sleep(15.0)\n\np.terminate()  # just in case, should not be necessary\n\n\nprint(\"\\n1.d. check the parameters of the extracted features\")\nurl = BASE_URL + '/feature-extraction/{}'.format(dsid)\n_print_url('GET', url)\nres = requests.get(url)\n\ndata = res.json()\nfor key, val in data.items():\n    if key!='filenames':\n           print('     - {}: {}'.format(key, val))\n\n\n# 2. Document categorization with ML algorithms\n\nprint(\"\\n2.a. Train the ML categorization model\")\nprint(\"       {} relevant, {} non-relevant files\".format(\n    len(relevant_files), len(non_relevant_files)))\nurl = BASE_URL + '/categorization/'\n_print_url(\"POST\", url)\n\nres = requests.post(url,\n                    json={'relevant_filenames': relevant_files,\n                          'non_relevant_filenames': non_relevant_files,\n                          'dataset_id': dsid,\n                          'method': 'LogisticRegression',  # one of \"LinearSVC\", \"LogisticRegression\", 'xgboost'\n                          'cv': 0  # use cross validation (recommended)\n                          })\n\ndata = res.json()\nmid = data['id']\nprint(\"     => model id = {}\".format(mid))\nprint('    => Training scores: MAP = {average_precision:.2f}, ROC-AUC = {roc_auc:.2f}'.format(**data))\n\nprint(\"\\n2.b. Check the parameters used in the categorization model\")\nurl = BASE_URL + '/categorization/{}'.format(mid)\n_print_url(\"GET\", url)\nres = requests.get(url)\n\ndata = res.json()\nfor key, val in data.items():\n    if \"filenames\" not in key:\n        print('     - {}: {}'.format(key, val))\n\nprint(\"\\n2.c Categorize the complete dataset with this model\")\nurl = BASE_URL + '/categorization/{}/predict'.format(mid)\n_print_url(\"GET\", url)\nres = requests.get(url)\nprediction = res.json()['prediction']\n\nprint(\"    => Predicting {} relevant and {} non relevant documents\".format(\n    len(list(filter(lambda x: x>0, prediction))),\n    len(list(filter(lambda x: x<0, prediction)))))\n\nprint(\"\\n2.d Test categorization accuracy\")\nprint(\"         using {}\".format(gtfile))  \nurl = BASE_URL + '/categorization/{}/test'.format(mid)\n_print_url(\"POST\", url)\nres = requests.post(url,\n                    json={'ground_truth_filename': gtfile})\n               \ndata2 = res.json()\nprint('    => Test scores: MAP = {average_precision:.2f}, ROC-AUC = {roc_auc:.2f}'.format(**data2))\n\n\n# 3. Document categorization with LSI\n\nprint(\"\\n3.a. Calculate LSI\")\n\nurl = BASE_URL + '/lsi/'\n_print_url(\"POST\", url)\n\nn_components = 100\nres = requests.post(url,\n                    json={'n_components': n_components,\n                          'dataset_id': dsid\n                          })\n\ndata = res.json()\nlid = data['id']\nprint('  => LSI model id = {}'.format(lid))\nprint('  => SVD decomposition with {} dimensions explaining {:.2f} % variabilty of the data'.format(\n                        n_components, data['explained_variance']*100))\nprint(\"\\n3.b. Predict categorization with LSI\")\n\nurl = BASE_URL + '/lsi/{}/predict'.format(lid)\n_print_url(\"POST\", url)\nres = requests.post(url,\n                    json={'relevant_filenames': relevant_files,\n                          'non_relevant_filenames': non_relevant_files\n                          })\ndata = res.json()\n\nprediction = data['prediction']\n\nprint('    => Training scores: MAP = {average_precision:.2f}, ROC-AUC = {roc_auc:.2f}'.format(**data))\n\n\nprint(\"\\n3.c. Test categorization with LSI\")\nurl = BASE_URL + '/lsi/{}/test'.format(lid)\n_print_url(\"POST\", url)\n\nres = requests.post(url,\n                    json={'relevant_filenames': relevant_files,\n                          'non_relevant_filenames': non_relevant_files,\n                          'ground_truth_filename': gtfile\n                          })\ndata2 = res.json()\nprint('    => Test scores: MAP = {average_precision:.2f}, ROC-AUC = {roc_auc:.2f}'.format(**data2))\n\npd.DataFrame({key: data[key] for key in data if 'prediction' in key or 'nearest' in key})\n\n\nprint(\"\\n4.a Delete the extracted features\")\nurl = BASE_URL + '/feature-extraction/{}'.format(dsid)\n_print_url(\"DELETE\", url)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.11", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}