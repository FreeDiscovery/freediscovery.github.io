{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nCategorization Example [REST API]\n---------------------------------\n\nAn example to illustrate binary categorizaiton with FreeDiscovery\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from __future__ import print_function\n\nfrom time import time, sleep\nfrom multiprocessing import Process\nimport requests\nimport pandas as pd\n\npd.options.display.float_format = '{:,.3f}'.format\npd.options.display.expand_frame_repr = False\n\ndataset_name = \"treclegal09_2k_subset\"     # see list of available datasets\n\nBASE_URL = \"http://localhost:5001/api/v0\"  # FreeDiscovery server URL\n\nif __name__ == '__main__':\n\n    print(\" 0. Load the test dataset\")\n    url = BASE_URL + '/datasets/{}'.format(dataset_name)\n    print(\" POST\", url)\n    res = requests.get(url).json()\n\n    # To use a custom dataset, simply specify the following variables\n    data_dir = res['data_dir']\n    seed_filenames = res['seed_filenames']\n    seed_y = res['seed_y']\n    ground_truth_file = res['ground_truth_file']  # (optional)\n\n\n    # 1. Feature extraction\n\n    print(\"\\n1.a Load dataset and initalize feature extraction\")\n    url = BASE_URL + '/feature-extraction'\n    print(\" POST\", url)\n    fe_opts = {'data_dir': data_dir,\n               'stop_words': 'english', 'chunk_size': 2000, 'n_jobs': -1,\n               'use_idf': 1, 'sublinear_tf': 0, 'binary': 0, 'n_features': 50001,\n               'analyzer': 'word', 'ngram_range': (1, 1), \"norm\": \"l2\"\n              }\n    res = requests.post(url, json=fe_opts).json()\n\n    dsid = res['id']\n    print(\"   => received {}\".format(list(res.keys())))\n    print(\"   => dsid = {}\".format(dsid))\n\n    print(\"\\n1.b Start feature extraction (in the background)\")\n\n    # Make this call in a background process (there should be a better way of doing it)\n    url = BASE_URL+'/feature-extraction/{}'.format(dsid)\n    print(\" POST\", url)\n    p = Process(target=requests.post, args=(url,))\n    p.start()\n    sleep(5.0) # wait a bit for the processing to start\n\n    print('\\n1.c Monitor feature extraction progress')\n    url = BASE_URL+'/feature-extraction/{}'.format(dsid)\n    print(\" GET\", url)\n\n    t0 = time()\n    while True:\n        res = requests.get(url)\n        if res.status_code == 520:\n            p.terminate()\n            raise ValueError('Processing did not start')\n        elif res.status_code == 200:\n            break # processing finished\n        data = res.json()\n        print('     ... {}k/{}k files processed in {:.1f} min'.format(\n                    data['n_samples_processed']//1000, data['n_samples']//1000, (time() - t0)/60.))\n        sleep(15.0)\n\n    p.terminate()  # just in case, should not be necessary\n\n\n    print(\"\\n1.d. check the parameters of the extracted features\")\n    url = BASE_URL + '/feature-extraction/{}'.format(dsid)\n    print(' GET', url)\n    res = requests.get(url).json()\n\n    print('\\n'.join(['     - {}: {}'.format(key, val) for key, val in res.items() \\\n                                                      if \"filenames\" not in key]))\n\n    method = BASE_URL + \"/feature-extraction/{}/index\".format(dsid)\n    res = requests.get(method, data={'filenames': seed_filenames})\n    seed_index = res.json()['index']\n\n    # 2. Document categorization with ML algorithms\n\n    print(\"\\n2.a. Train the ML categorization model\")\n    print(\"   {} relevant, {} non-relevant files\".format(seed_y.count(1), seed_y.count(0)))\n    url = BASE_URL + '/categorization/'\n    print(\" POST\", url)\n    print(' Training...')\n\n    res = requests.post(url,\n                        json={'index': seed_index,\n                              'y': seed_y,\n                              'dataset_id': dsid,\n                              'method': 'LinearSVC',  # one of \"LinearSVC\", \"LogisticRegression\", 'xgboost'\n                              'cv': 0                          # Cross Validation\n                              }).json()\n\n    mid = res['id']\n    print(\"     => model id = {}\".format(mid))\n    print('    => Training scores: MAP = {average_precision:.3f}, ROC-AUC = {roc_auc:.3f}'.format(**res))\n\n    print(\"\\n2.b. Check the parameters used in the categorization model\")\n    url = BASE_URL + '/categorization/{}'.format(mid)\n    print(\" GET\", url)\n    res = requests.get(url).json()\n\n    print('\\n'.join(['     - {}: {}'.format(key, val) for key, val in res.items() \\\n                                                      if key not in ['index', 'y']]))\n\n    print(\"\\n2.c Categorize the complete dataset with this model\")\n    url = BASE_URL + '/categorization/{}/predict'.format(mid)\n    print(\" GET\", url)\n    res = requests.get(url).json()\n    prediction = res['prediction']\n\n    print(\"    => Predicting {} relevant and {} non relevant documents\".format(\n        len(list(filter(lambda x: x>0, prediction))),\n        len(list(filter(lambda x: x<0, prediction)))))\n\n    print(\"\\n2.d Test categorization accuracy\")\n    print(\"         using {}\".format(ground_truth_file))  \n    url = BASE_URL + '/categorization/{}/test'.format(mid)\n    print(\"POST\", url)\n    res = requests.post(url, json={'ground_truth_filename': ground_truth_file}).json()\n\n    print('    => Test scores: MAP = {average_precision:.3f}, ROC-AUC = {roc_auc:.3f}'.format(**res))\n\n\n    # 3. Document categorization with LSI\n\n    print(\"\\n3.a. Calculate LSI\")\n\n    url = BASE_URL + '/lsi/'\n    print(\"POST\", url)\n\n    n_components = 100\n    res = requests.post(url,\n                        json={'n_components': n_components,\n                              'dataset_id': dsid\n                              }).json()\n\n    lid = res['id']\n    print('  => LSI model id = {}'.format(lid))\n    print('  => SVD decomposition with {} dimensions explaining {:.2f} % variabilty of the data'.format(\n                            n_components, res['explained_variance']*100))\n    print(\"\\n3.b. Predict categorization with LSI\")\n\n    url = BASE_URL + '/lsi/{}/predict'.format(lid)\n    print(\"POST\", url)\n    res = requests.post(url,\n                        json={'index': seed_index,\n                              'y': seed_y\n                              }).json()\n    prediction = res['prediction']\n\n    print('    => Training scores: MAP = {average_precision:.3f}, ROC-AUC = {roc_auc:.3f}'.format(**res))\n    df = pd.DataFrame({key: res[key] for key in res if 'prediction'==key or 'nearest' in key})\n\n\n    print(\"\\n3.c. Test categorization with LSI\")\n    url = BASE_URL + '/lsi/{}/test'.format(lid)\n    print(\" POST\", url)\n\n    res = requests.post(url,\n                        json={'index': seed_index,\n                              'y': seed_y,\n                              'ground_truth_filename': ground_truth_file\n                              }).json()\n    print(res)\n    print('    => Test scores: MAP = {average_precision:.3f}, ROC-AUC = {roc_auc:.3f}'.format(**res))\n\n    print('\\n', df)\n\n\n    print(\"\\n4.a Delete the extracted features\")\n    url = BASE_URL + '/feature-extraction/{}'.format(dsid)\n    print(\" DELETE\", url)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.11", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}