

.. _sphx_glr_examples_REST_semantic_search.py:


Semantic Search Example [REST API]
----------------------------------

An example of Semantic Search





.. rst-class:: sphx-glr-script-out

 Out::

    0. Load the test dataset
     GET http://localhost:5001/api/v0/example-dataset/treclegal09_2k_subset

    1.a Load dataset and initalize feature extraction
     POST http://localhost:5001/api/v0/feature-extraction
       => received ['filenames', 'id']
       => dsid = cc80c848a9a24146

    1.b Start feature extraction
     POST http://localhost:5001/api/v0/feature-extraction/cc80c848a9a24146

    2. Calculate LSI
    POST http://localhost:5001/api/v0/lsi/
      => LSI model id = 91bb4d9437174d20
      => SVD decomposition with 100 dimensions explaining 76.47 % variabilty of the data

    3.a. Perform the semantic search
     POST http://localhost:5001/api/v0/search/
                 score
    document_id       
    4528384      0.454
    21316        0.429
    1572516      0.423
    4116841      0.404
    2152089      0.403
    1550025      0.402
    648025       0.400
    649636       0.399
    2856100      0.396
    674041       0.396
    776161       0.395
    1682209      0.395
    1454436      0.389
    3545689      0.388
    202500       0.386
    1976836      0.384
    777924       0.382
    68644        0.382
    774400       0.382
    1684804      0.380
    558009       0.380
    4363921      0.379
    69169        0.379
    3225616      0.378
    4923961      0.376
    6724         0.373
    21609        0.368
    8836         0.367
    4393216      0.367
    1048576      0.367
    ...            ...
    4293184      0.179
    1040400      0.178
    2050624      0.178
    1607824      0.178
    4410000      0.178
    122500       0.178
    2277081      0.178
    908209       0.177
    2280100      0.177
    114921       0.175
    898704       0.175
    708964       0.175
    1610361      0.175
    101761       0.174
    385641       0.174
    3312400      0.173
    872356       0.173
    243049       0.172
    904401       0.172
    906304       0.171
    1311025      0.171
    1085764      0.169
    5085025      0.168
    1083681      0.167
    495616       0.165
    5143824      0.163
    2268036      0.159
    614656       0.158
    1510441      0.158
    2271049      0.150

    [2465 rows x 1 columns]
    0.453811778093

    4. Delete the extracted features
     DELETE http://localhost:5001/api/v0/feature-extraction/cc80c848a9a24146




|


.. code-block:: python


    from __future__ import print_function

    import os.path
    import requests
    import pandas as pd

    pd.options.display.float_format = '{:,.3f}'.format
    pd.options.display.expand_frame_repr = False

    dataset_name = "treclegal09_2k_subset"     # see list of available datasets

    BASE_URL = "http://localhost:5001/api/v0"  # FreeDiscovery server URL

    if __name__ == '__main__':

        print(" 0. Load the test dataset")
        url = BASE_URL + '/example-dataset/{}'.format(dataset_name)
        print(" GET", url)
        input_ds = requests.get(url).json()

        # create a custom dataset definition for ingestion
        data_dir = input_ds['metadata']['data_dir']
        dataset_definition = [{'document_id': row['document_id'],
                               'file_path': os.path.join(data_dir, row['file_path'])} \
                                       for row in input_ds['dataset']]

        # 1. Feature extraction

        print("\n1.a Load dataset and initalize feature extraction")
        url = BASE_URL + '/feature-extraction'
        print(" POST", url)
        res = requests.post(url, json={'dataset_definition': dataset_definition,
                                       'use_hashing': True}).json()

        dsid = res['id']
        print("   => received {}".format(list(res.keys())))
        print("   => dsid = {}".format(dsid))

        print("\n1.b Start feature extraction")

        url = BASE_URL+'/feature-extraction/{}'.format(dsid)
        print(" POST", url)
        requests.post(url)

        # 3. Document categorization with LSI (used for Nearest Neighbors method)

        print("\n2. Calculate LSI")

        url = BASE_URL + '/lsi/'
        print("POST", url)

        n_components = 100
        res = requests.post(url,
                            json={'n_components': n_components,
                                  'parent_id': dsid
                                  }).json()

        lsi_id = res['id']
        print('  => LSI model id = {}'.format(lsi_id))
        print('  => SVD decomposition with {} dimensions explaining {:.2f} % variabilty of the data'.format(
                                n_components, res['explained_variance']*100))


        # 3. Semantic search

        print("\n3.a. Perform the semantic search")


        query = """There are some conflicts with the draft date, so we will probably need to
                    have it on a different date."""

        url = BASE_URL + '/search/'
        print(" POST", url)

        res = requests.post(url,
                            json={'parent_id': lsi_id,
                                  'query': query
                                  }).json()

        data = res['data']

        df = pd.DataFrame(data).set_index('document_id')
        print(df)

        print(df.score.max())


        # 4. Cleaning
        print("\n4. Delete the extracted features")
        url = BASE_URL + '/feature-extraction/{}'.format(dsid)
        print(" DELETE", url)
        requests.delete(url)

**Total running time of the script:** ( 0 minutes  11.778 seconds)



.. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: REST_semantic_search.py <REST_semantic_search.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: REST_semantic_search.ipynb <REST_semantic_search.ipynb>`

.. rst-class:: sphx-glr-signature

    `Generated by Sphinx-Gallery <http://sphinx-gallery.readthedocs.io>`_
