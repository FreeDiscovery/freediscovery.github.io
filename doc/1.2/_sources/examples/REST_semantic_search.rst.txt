

.. _sphx_glr_examples_REST_semantic_search.py:


Semantic Search Example [REST API]
----------------------------------

An example of Semantic Search





.. rst-class:: sphx-glr-script-out

 Out::

    0. Load the test dataset
     GET http://localhost:5001/api/v0/example-dataset/treclegal09_2k_subset

    1.a Load dataset and initalize feature extraction
     POST http://localhost:5001/api/v0/feature-extraction
       => received ['id']
       => dsid = ee6be72762e0437e

    1.b Start feature extraction
     POST http://localhost:5001/api/v0/feature-extraction/ee6be72762e0437e

    2. Calculate LSI
    POST http://localhost:5001/api/v0/lsi/
      => LSI model id = e05903cc5b7b4015
      => SVD decomposition with 100 dimensions explaining 69.79 % variabilty of the data

    3.a. Perform the semantic search
     POST http://localhost:5001/api/v0/search/
                 score
    document_id       
    5267025      0.557
    5262436      0.551
    115600       0.464
    5148361      0.464
    116964       0.461
    5157441      0.461
    5152900      0.417
    116281       0.417
    3541924      0.372
    3175524      0.369
    1028196      0.367
    4950625      0.366
    4813636      0.366
    75625        0.366
    73984        0.366
    54756        0.366
    5067001      0.364
    97969        0.364
    4800481      0.363
    53361        0.363
    1181569      0.356
    2500         0.309
    4251844      0.309
    145161       0.297
    1481089      0.292
    5546025      0.284
    5253264      0.284
    3625216      0.279
    3617604      0.279
    1218816      0.279
    ...            ...
    38809       -0.180
    3694084     -0.182
    4635409     -0.183
    33856       -0.183
    732736      -0.185
    4822416     -0.186
    66049       -0.186
    889249      -0.187
    249001      -0.191
    277729      -0.192
    4910656     -0.198
    66564       -0.200
    4669921     -0.200
    734449      -0.203
    577600      -0.205
    1640961     -0.206
    4884100     -0.220
    62500       -0.220
    954529      -0.243
    3125824     -0.244
    2621161     -0.247
    3132900     -0.251
    3118756     -0.252
    3139984     -0.253
    956484      -0.254
    3115225     -0.259
    3129361     -0.262
    3122289     -0.262
    3136441     -0.263
    193600      -0.265

    [2465 rows x 1 columns]
    0.557092923094

    4. Delete the extracted features
     DELETE http://localhost:5001/api/v0/feature-extraction/ee6be72762e0437e




|


.. code-block:: python


    from __future__ import print_function

    import os.path
    import requests
    import pandas as pd

    pd.options.display.float_format = '{:,.3f}'.format
    pd.options.display.expand_frame_repr = False

    dataset_name = "treclegal09_2k_subset"     # see list of available datasets

    BASE_URL = "http://localhost:5001/api/v0"  # FreeDiscovery server URL

    if __name__ == '__main__':

        print(" 0. Load the test dataset")
        url = BASE_URL + '/example-dataset/{}'.format(dataset_name)
        print(" GET", url)
        input_ds = requests.get(url).json()

        # create a custom dataset definition for ingestion
        data_dir = input_ds['metadata']['data_dir']
        dataset_definition = [{'document_id': row['document_id'],
                               'file_path': os.path.join(data_dir, row['file_path'])}
                              for row in input_ds['dataset']]

        # 1. Feature extraction

        print("\n1.a Load dataset and initalize feature extraction")
        url = BASE_URL + '/feature-extraction'
        print(" POST", url)
        res = requests.post(url).json()

        dsid = res['id']
        print("   => received {}".format(list(res.keys())))
        print("   => dsid = {}".format(dsid))

        print("\n1.b Start feature extraction")

        url = BASE_URL+'/feature-extraction/{}'.format(dsid)
        print(" POST", url)
        requests.post(url, json={'dataset_definition': dataset_definition})

        # 3. Document categorization with LSI (used for Nearest Neighbors method)

        print("\n2. Calculate LSI")

        url = BASE_URL + '/lsi/'
        print("POST", url)

        n_components = 100
        res = requests.post(url,
                            json={'n_components': n_components,
                                  'parent_id': dsid
                                  }).json()

        lsi_id = res['id']
        print('  => LSI model id = {}'.format(lsi_id))
        print('  => SVD decomposition with {} dimensions explaining {:.2f} % variabilty of the data'.format(
                                n_components, res['explained_variance']*100))


        # 3. Semantic search

        print("\n3.a. Perform the semantic search")


        query = """There are some conflicts with the draft date, so we will probably need to
                    have it on a different date."""

        url = BASE_URL + '/search/'
        print(" POST", url)

        res = requests.post(url,
                            json={'parent_id': lsi_id,
                                  'query': query
                                  }).json()

        data = res['data']

        df = pd.DataFrame(data).set_index('document_id')
        print(df)

        print(df.score.max())


        # 4. Cleaning
        print("\n4. Delete the extracted features")
        url = BASE_URL + '/feature-extraction/{}'.format(dsid)
        print(" DELETE", url)
        requests.delete(url)

**Total running time of the script:** ( 0 minutes  4.143 seconds)



.. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: REST_semantic_search.py <REST_semantic_search.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: REST_semantic_search.ipynb <REST_semantic_search.ipynb>`

.. rst-class:: sphx-glr-signature

    `Generated by Sphinx-Gallery <http://sphinx-gallery.readthedocs.io>`_
